import streamlit as st
import sys
import os
import random
import time
from openai import OpenAI

# ===========================
# 0. ç¯å¢ƒè·¯å¾„ä¿®å¤
# ===========================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.abspath(os.path.join(current_dir, '..'))
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from engine_manager import init_data, render_sidebar
from style_manager import apply_pro_style

# ===========================
# 1. åˆå§‹åŒ–
# ===========================
st.set_page_config(layout="wide", page_title="Graphic Lab")
apply_pro_style()
render_sidebar()
init_data()

client = None
if "DEEPSEEK_KEY" in st.secrets:
    try:
        client = OpenAI(api_key=st.secrets["DEEPSEEK_KEY"], base_url="https://api.deepseek.com")
    except:
        pass

# ==========================================
# 2. æ ¸å¿ƒå¼•æ“ (ä¸¥æ ¼å¤åˆ» 9+1 é…æ–™é€»è¾‘)
# ==========================================
def smart_pick(category):
    db = st.session_state.get("db_all", {})
    items = db.get(category, [])
    if items: return random.choice(items)
    return ""

def assemble_skeleton_fixed(user_input):
    """
    ã€æ ¸å¿ƒé€»è¾‘å ¡å’ - ç»ä¸é˜‰å‰²ç‰ˆã€‘
    ä¸¥æ ¼éµå®ˆï¼šIntent -> Subject -> Style -> Tech -> Color -> Texture -> Comp -> Action -> Mood -> Accent -> Usage
    """
    # 1. å¤‡æ–™ (9ä¸ªæ ¸å¿ƒé…æ–™ + 1ä¸ªç‚¹ç¼€)
    sub     = smart_pick("Subject")
    s_sys   = smart_pick("StyleSystem")
    s_tech  = smart_pick("Technique")
    s_col   = smart_pick("Color")
    s_tex   = smart_pick("Texture")
    s_comp  = smart_pick("Composition")
    act     = smart_pick("Action")
    mood    = smart_pick("Mood")
    usage   = smart_pick("Usage")
    s_acc   = smart_pick("Accent") # å°†ç‚¹ç¼€å˜ä¸ºå¯æ§é¡¹

    # 2. ç¡®å®šæ ¸å¿ƒä¸»ä½“ (å¦‚æœç”¨æˆ·æ²¡è¾“å…¥ï¼Œåˆ™ä» Subject æŠ½)
    final_subject = user_input.strip() if user_input.strip() else sub
    
    # 3. ç»„è£…é“¾æ¡ (ä¸¥æ ¼æŒ‰ç…§ 01_creative.py çš„ Sequence)
    parts = [
        final_subject,                 
        f"{s_sys} style" if s_sys else "",               
        f"{s_tech} technique" if s_tech else "",              
        f"{s_col} palette" if s_col else "",               
        f"{s_tex} texture" if s_tex else "",               
        f"{s_comp} composition" if s_comp else "",              
        act,                 
        f"{mood} vibe" if mood else "",
        f"with {s_acc} details" if s_acc else "" # å–æ¶ˆ 40% éšæœºï¼Œæœ‰å°±åŠ ä¸Š
    ]

    # 4. ç”Ÿæˆåˆæ­¥é“¾æ¡
    raw_chain = "ï¼Œ".join([p for p in parts if p])
    
    # 5. å¤„ç† Usage (ä¸¥æ ¼å¤åˆ»â€œçº¹åœ¨...â€é€»è¾‘)
    if usage:
        raw_chain += f"ï¼Œçº¹åœ¨{usage}"
        
    return raw_chain

# ===========================
# 3. ç•Œé¢äº¤äº’
# ===========================
st.markdown("## ğŸ¨ Graphic Lab")
st.caption("Auto-Assembly -> AI Polish -> Batch Handoff")

c1, c2 = st.columns([3, 1])
with c1:
    user_in = st.text_input("Core Idea / Subject", placeholder="åœ¨æ­¤è¾“å…¥æ ¸å¿ƒåˆ›æ„æˆ–ä¸»ä½“...", label_visibility="collapsed")
with c2:
    qty = st.number_input("Batch Size", 1, 8, 4, label_visibility="collapsed")

# ===========================
# 4. æ‰§è¡Œé€»è¾‘
# ===========================
if st.button("Generate", type="primary", use_container_width=True):
    
    st.session_state.graphic_solutions = [] 
    placeholders = []   
    skeletons = []      
    subject_anchors = [] # è®°å½•ä¸»ä½“ç”¨äº AI é”æ­»
    
    # --- ç¬¬ä¸€é˜¶æ®µï¼šéª¨æ¶æˆå‹ ---
    for i in range(qty):
        idx = i + 1
        ph = st.empty()
        placeholders.append(ph)
        
        sk = assemble_skeleton_fixed(user_in)
        skeletons.append(sk)
        
        # æå–ç¬¬ä¸€ä¸ªé€—å·å‰çš„è¯ä½œä¸º Subject é”šç‚¹
        anchor = sk.split('ï¼Œ')[0].strip()
        subject_anchors.append(anchor)
        
        with ph.container(border=True):
            st.markdown(f"**æ–¹æ¡ˆ{idx}ï¼š** {sk}")
            st.caption("âœ¨ èµ„æ·±ç­–å±•äººæ­£åœ¨æ¶¦è‰²æ–‡æ¡ˆ...") 
    
    # --- ç¬¬äºŒé˜¶æ®µï¼šAI è‰ºæœ¯åŒ–æ¶¦è‰² (è¿˜åŸè°ƒæ•™é€»è¾‘) ---
    sys_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±åˆºé’ç­–å±•äººã€‚è¯·å°†æä¾›çš„å…³é”®è¯ç»„åˆæ¶¦è‰²ä¸ºæå…·è‰ºæœ¯æ„Ÿçš„çº¹èº«æè¿°ã€‚æ¯æ®µå¿…é¡»å‡ºç°'çº¹èº«'äºŒå­—ã€‚"
    
    final_results = []

    for i, sk in enumerate(skeletons):
        idx = i + 1
        ph = placeholders[i]
        anchor = subject_anchors[i]
        
        user_prompt = f"""
        ã€åŸå§‹éª¨æ¶ã€‘ï¼š{sk}
        ã€æ ¸å¿ƒä¸»ä½“ã€‘ï¼š{anchor}
        
        ã€æŒ‡ä»¤ã€‘ï¼š
        1. å¿…é¡»åœ¨æè¿°ä¸­â€œå­—é¢ä¿ç•™â€æ ¸å¿ƒä¸»ä½“ï¼š{anchor}ã€‚
        2. å¿…é¡»ä¸¥æ ¼ä¿ç•™éª¨æ¶ä¸­çš„é£æ ¼ã€é¢œè‰²ã€éƒ¨ä½æè¿°ã€‚
        3. å¿…é¡»ä¸¥æ ¼ä»¥ "**æ–¹æ¡ˆ{idx}ï¼š**" å¼€å¤´ã€‚
        4. è¾“å‡ºä¸€æ®µ 60-90 å­—çš„å®Œæ•´è§†è§‰æè¿°ã€‚
        """
        
        full_response = ""
        try:
            ph.empty()
            with ph.container(border=True):
                if client:
                    stream = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[{"role": "system", "content": sys_prompt},{"role": "user", "content": user_prompt}],
                        temperature=0.85, 
                        stream=True 
                    )
                    full_response = st.write_stream(stream)
                    
                    if not full_response.startswith(f"**æ–¹æ¡ˆ{idx}ï¼š**"):
                        full_response = f"**æ–¹æ¡ˆ{idx}ï¼š** {full_response}"
                    
                    # å¼ºæ ¡éªŒï¼šå¦‚æœä¸»ä½“è¢«æ¶¦è‰²ä¸¢äº†ï¼Œè¡¥å›æ¥
                    if anchor not in full_response:
                        full_response = full_response.replace(f"**æ–¹æ¡ˆ{idx}ï¼š**", f"**æ–¹æ¡ˆ{idx}ï¼š** å›´ç»•ç€ã€{anchor}ã€‘å±•å¼€çš„çº¹èº«")
                else:
                    full_response = f"**æ–¹æ¡ˆ{idx}ï¼š** {sk} (Offline Mode)"
                    st.write(full_response)
        except Exception as e:
            full_response = f"**æ–¹æ¡ˆ{idx}ï¼š** {sk} (Error: {str(e)})"
            ph.markdown(full_response)

        final_results.append(full_response)

    st.session_state.graphic_solutions = final_results
    st.rerun()

# ===========================
# 5. ç»“æœå±•ç¤ºä¸å åŠ å‘é€
# ===========================
if "graphic_solutions" in st.session_state and st.session_state.graphic_solutions:
    st.markdown("---")
    st.subheader("ğŸ“¦ Ready for Automation")
    
    for sol in st.session_state.graphic_solutions:
        with st.container(border=True):
            st.markdown(sol)
        
    c_send, c_clear = st.columns([3, 1])
    
    with c_send:
        if st.button("ğŸš€ Send ALL to Automation Pipeline (å åŠ )", type="primary", use_container_width=True):
            if "global_queue" not in st.session_state:
                st.session_state.global_queue = []
            st.session_state.global_queue.extend(st.session_state.graphic_solutions)
            st.toast(f"âœ… å·²æ·»åŠ  {len(st.session_state.graphic_solutions)} ç»„æ–¹æ¡ˆ")
            time.sleep(0.8)
            st.switch_page("pages/03_ğŸš€_Automation.py")
            
    with c_clear:
        if st.button("ğŸ—‘ï¸ Clear Results", use_container_width=True):
            st.session_state.graphic_solutions = []
            st.rerun()
